# Finetuning-BERT-model
BERT uses a Transformer-based encoder only architecture, and is pre-trained on large unlabeled text corpora using masked language modeling (MLM) and next-sentence prediction (NSP)
